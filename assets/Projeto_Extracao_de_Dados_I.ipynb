{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "832b40d8-8b1c-448e-9004-819334ce0710",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Projeto - Extração de Dados I\n",
    "\n",
    "### Sistema de Monitoramento de Avanços no Campo da Genômica\n",
    "\n",
    "### Contexto:\n",
    "\n",
    "#### O grupo trabalha no time de engenharia de dados na HealthGen, uma empresa especializada em genômica e pesquisa de medicina personalizada. A genômica é o estudo do conjunto completo de genes de um organismo, desempenha um papel fundamental na medicina personalizada e na pesquisa biomédica. Permite a análise do DNA para identificar variantes genéticas e mutações associadas a doenças e facilita a personalização de tratamentos com base nas características genéticas individuais dos pacientes.\n",
    "\n",
    "#### A empresa precisa se manter atualizada sobre os avanços mais recentes na genômica, identificar oportunidades para pesquisa e desenvolvimento de tratamentos personalizados e acompanhar as tendências em genômica que podem influenciar estratégias de pesquisa e desenvolvimento. Pensando nisso, o time de dados apresentou uma proposta de desenvolvimento de um sistema que coleta, analisa e apresenta as últimas notícias relacionadas à genômica e à medicina personalizada, e também estuda o avanço do campo nos últimos anos. \n",
    "\n",
    "#### O time de engenharia de dados tem como objetivo desenvolver e garantir um pipeline de dados confiável e estável. As principais atividades são:\n",
    "\n",
    "> #### 1. Consumo de dados com a News API: \n",
    "> #### Implementar um mecanismo para consumir dados de notícias de fontes confiáveis e especializadas em genômica e medicina personalizada, a partir da News API: \n",
    "https://newsapi.org/\n",
    "\n",
    "> #### 2. Definir Critérios de Relevância:\n",
    "\n",
    "> #### Desenvolver critérios precisos de relevância para filtrar as notícias. Por exemplo, o time pode se concentrar em notícias que mencionem avanços em sequenciamento de DNA, terapias genéticas personalizadas ou descobertas relacionadas a doenças genéticas específicas.\n",
    "\n",
    "> #### 3. Cargas em Batches:\n",
    "\n",
    "> #### Armazenar as notícias relevantes em um formato estruturado e facilmente acessível para consultas e análises posteriores. Essa carga deve acontecer 1 vez por hora. Se as notícias extraídas já tiverem sidos armazenadas na carga anterior, o processo deve ignorar e não armazenar as notícias novamente, os dados carregados não podem ficar duplicados.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./parte1.png\"  width=\"70%\" height=\"40%\">\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</div>\n",
    "\n",
    "> #### 4. Dados transformados para consulta do público final\n",
    "\n",
    "> #### A partir dos dados carregados, aplicar as seguintes transformações e armazenar o resultado final para a consulta do público final:\n",
    "\n",
    "> #### 4.1 - Quantidade de notícias por ano, mês e dia de publicação;\n",
    "> #### 4.2 - Quantidade de notícias por fonte e autor;\n",
    "> #### 4.3 - Quantidade de aparições de 3 palavras chaves por ano, mês e dia de publicação (as 3 palavras chaves serão as mesmas usadas para fazer os filtros de relevância do item 2 (2. Definir Critérios de Relevância)).\n",
    "\n",
    "> #### Atualizar os dados transformados 1 vez por dia.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./parte2.png\"  width=\"70%\" height=\"40%\">\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</div>\n",
    "\n",
    "### Além das atividades principais, existe a necessidade de busca de dados por eventos em tempo real quando é necessário, para isso foi desenhado duas opções:\n",
    "\n",
    "> #### Opção 1 - Apache Kafka e Spark Streaming:\n",
    "\n",
    "> #### Preparar um pipeline com Apache Kafka e Spark Streaming para receber os dados do Produtor Kafka representado por um evento manual e consumir os dados com o Spark Streaming armazenando os resultados temporariamente. Em um processo paralelo, verificar os resultados armazenados temporiamente e armazenar no mesmo destino do item 3 (3. Cargas em Batches) aqueles resultados que ainda não foram armazenados no destino (os dados carregados não podem ficar duplicados). E por fim, eliminar os dados temporários após a verificação e a eventual carga.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./opcao_1.png\"  width=\"70%\" height=\"40%\">\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "> #### Opção 2 - Webhooks com notificações por eventos:\n",
    "\n",
    "> #### Configurar um webhook para adquirir as últimas notícias a partir de um evento representado por uma requisição POST e fazer a chamada da API e por fim armazenar os resultados temporariamente. Em um processo paralelo, verificar os resultados armazenados temporiamente e armazenar no mesmo destino do item 3 (3. Cargas em Batches) aqueles resultados que ainda não foram armazenados no destino (os dados carregados não podem ficar duplicados). E por fim, eliminar os dados temporários após a verificação e a eventual carga.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./opcao_2.png\"  width=\"70%\" height=\"40%\">\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</div>\n",
    "\n",
    "## Atividades que precisam ser realizadas pelo grupo:\n",
    "\n",
    "#### O grupo precisa construir o pipeline de dados seguindo os requisitos das atividades principais e escolher entre a Opção 1 e Opção 2 para desenvolvimento."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Projeto_Extracao_de_Dados_I",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
